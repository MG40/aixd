{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import necessary libraries\n","import boto3, re, sys, math, json, os\n","import numpy as np \n","import io as io\n","import pandas as pd "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install python-dotenv\n","%reload_ext dotenv\n","%dotenv"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Access and Credential details\n","bucket_name = os.environ['BUCKET_NAME'] # <--- Environment variable for Bucket Name\n","ecs_access_key_id=os.environ['ECS_ACCESS_KEY_ID']  # <--- Environment variable for ECS Access Key\n","ecs_secret_access_key=os.environ['ECS_SECRET_ACCESS_KEY'] # <--- Environment variable for Secret Access Key\n","endpoint_url=os.environ['ENDPOINT_URL'] # <--- Environment variable for Endpoint URL"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["s3 = boto3.resource(service_name='s3',aws_access_key_id=ecs_access_key_id,aws_secret_access_key=ecs_secret_access_key,endpoint_url=endpoint_url)\n","\n","for bucket in s3.buckets.all():\n","  print(bucket.name)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["s3_client = boto3.client(service_name='s3',aws_access_key_id=ecs_access_key_id,aws_secret_access_key=ecs_secret_access_key,endpoint_url=endpoint_url)\n","\n","from_aws = \"sagemaker-created-me\"\n","\n","evaluate=s3_client.get_object(Bucket=from_aws, Key = 'bank_clean.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["try: \n","    model_data = pd.read_csv(io.BytesIO(evaluate['Body'].read()),index_col=0) \n","    print('Success: Data loaded into dataframe.') \n","except Exception as e: \n","    print('Data load error: ',e)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data))]) \n","print(train_data.shape, test_data.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Upload file to ECS bucket with \"upload_\" prefix \n","s3=boto3.resource(service_name='s3',aws_access_key_id=ecs_access_key_id,aws_secret_access_key=ecs_secret_access_key,endpoint_url=endpoint_url) \n","upload_train=s3.Bucket(bucket_name).Object(os.path.join('train/azure_train.csv')).upload_file('train.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["download_train=s3.Bucket(bucket_name).Object(os.path.join('train/azure_train.csv')).download_file('download_train.csv') \n","# Check in the terminal for downloaded file \n","# You should be able to see download_train.csv and train.csv"]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
